{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training data using a sparse vectorizer\n",
      "n_samples: 11314, n_features: 101322\n",
      "()\n",
      "n_samples: 7532, n_features: 101322\n",
      "()\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.126s\n",
      "test time:  0.045s\n",
      "accuracy:   0.696\n",
      "dimensionality: 101322\n",
      "density: 1.000000\n",
      "classification report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.56      0.43      0.49       319\n",
      "           comp.graphics       0.65      0.71      0.68       389\n",
      " comp.os.ms-windows.misc       0.75      0.46      0.57       394\n",
      "comp.sys.ibm.pc.hardware       0.59      0.72      0.65       392\n",
      "   comp.sys.mac.hardware       0.71      0.69      0.70       385\n",
      "          comp.windows.x       0.79      0.75      0.77       395\n",
      "            misc.forsale       0.81      0.72      0.76       390\n",
      "               rec.autos       0.76      0.72      0.74       396\n",
      "         rec.motorcycles       0.76      0.73      0.74       398\n",
      "      rec.sport.baseball       0.94      0.81      0.87       397\n",
      "        rec.sport.hockey       0.59      0.93      0.72       399\n",
      "               sci.crypt       0.72      0.76      0.74       396\n",
      "         sci.electronics       0.74      0.60      0.66       393\n",
      "                 sci.med       0.84      0.77      0.80       396\n",
      "               sci.space       0.75      0.80      0.78       394\n",
      "  soc.religion.christian       0.59      0.86      0.70       398\n",
      "      talk.politics.guns       0.56      0.72      0.63       364\n",
      "   talk.politics.mideast       0.81      0.80      0.80       376\n",
      "      talk.politics.misc       0.56      0.44      0.49       310\n",
      "      talk.religion.misc       0.47      0.21      0.29       251\n",
      "\n",
      "             avg / total       0.70      0.70      0.69      7532\n",
      "\n",
      "confusion matrix:\n",
      "[[138   1   2   2   2   2   0   3   3   2  12   3   0   3  10  74  13  12\n",
      "   12  25]\n",
      " [  2 278   6  17  15  24   6   0   5   3   6  13   2   0   9   2   0   1\n",
      "    0   0]\n",
      " [  4  30 181  73  16  30   4   2   5   0  16  12   3   2   8   1   0   1\n",
      "    3   3]\n",
      " [  0  13  20 284  29   4  10   2   0   0   8   5  16   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0  11   8  32 265   5   8   7   2   0  14   7  13   2   6   2   1   1\n",
      "    1   0]\n",
      " [  0  46   9   9   7 297   1   0   0   1   7   5   5   3   4   0   1   0\n",
      "    0   0]\n",
      " [  1   4   1  32  18   0 281  14   7   3  11   1   7   2   4   1   1   0\n",
      "    2   0]\n",
      " [  1   1   1   1   1   0   9 285  31   1  25   4  10   1   6   3   4   3\n",
      "    9   0]\n",
      " [  8   3   1   0   2   3   7  26 290   1  15   0   8   3   6   3  11   3\n",
      "    7   1]\n",
      " [  5   2   0   0   0   1   6   0   4 322  34   4   1   3   2   3   5   1\n",
      "    4   0]\n",
      " [  5   0   0   0   0   1   0   1   3   3 373   3   0   2   2   4   2   0\n",
      "    0   0]\n",
      " [  1   9   6   3   4   2   1   0   3   2  18 301   3   1   5   3  20   4\n",
      "    9   1]\n",
      " [  1  11   6  27  12   0  10  11   6   1  11  33 237  11  10   2   0   2\n",
      "    2   0]\n",
      " [  4   5   0   1   0   0   3   7   4   0  15   0   5 304  11  17   9   4\n",
      "    5   2]\n",
      " [  4   6   1   1   0   2   1   6   2   1  18   2   5   4 317   5   3   7\n",
      "    8   1]\n",
      " [  8   3   0   1   1   2   0   1   1   1  15   1   0   2   1 343   4   0\n",
      "    3  11]\n",
      " [  5   0   0   0   0   1   1   3   5   1  12  11   1   5   8  11 262   8\n",
      "   16  14]\n",
      " [ 11   3   0   1   0   1   0   2   4   2   9   2   0   0   1  14  10 299\n",
      "   17   0]\n",
      " [ 14   2   0   0   0   3   1   4   3   0   8   6   2   7   8   8  94  12\n",
      "  136   2]\n",
      " [ 34   3   0   1   0   0   0   2   4   0   8   4   2   7   5  87  25   9\n",
      "    7  53]]\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('MultinomialNB',\n",
       " 0.69649495485926716,\n",
       " 0.12599992752075195,\n",
       " 0.04499983787536621)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "categories = None\n",
    "\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "    \n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "target_names = data_train.target_names\n",
    "\n",
    "# split a training set and a test set\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "X_train = vectorizer.fit_transform(data_train.data)\n",
    "\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()\n",
    "\n",
    "# mapping from integer feature name to original token string\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "\n",
    "\n",
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\"\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Benchmark classifiers\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "    \n",
    "    s = pickle.dumps(clf)\n",
    "    \n",
    "    joblib.dump(clf, 'filename.pkl') \n",
    "    \n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "   \n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(y_test, pred,\n",
    "                                        target_names=target_names))\n",
    "\n",
    "\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n",
    "\n",
    "results = []\n",
    "\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "\n",
    "\n",
    "benchmark(MultinomialNB(alpha=.01))\n",
    "# results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
